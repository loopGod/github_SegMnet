{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['“', '人们', '常', '说', '生活', '是', '一', '部', '教科书', '，', '而', '血', '与', '火', '的', '战争', '更', '是', '不可多得', '的', '教科书', '，', '她', '确实', '是', '名副其实', '的', '‘', '我', '的', '大学', '’', '。'], ['“', '心', '静', '渐', '知', '春', '似', '海', '，', '花', '深', '每', '觉', '影', '生', '香', '。'], ['“', '吃', '屎', '的', '东西', '，', '连', '一', '捆', '麦', '也', '铡', '不', '动', '呀', '？'], ['他', '“', '严格要求', '自己', '，', '从', '一个', '科举', '出身', '的', '进士', '成为', '一个', '伟大', '的', '民主主义', '者', '，', '进而', '成为', '一', '位', '杰出', '的', '党外', '共产主义', '战士', '，', '献身', '于', '崇高', '的', '共产主义', '事业', '。'], ['“', '征', '而', '未', '用', '的', '耕地', '和', '有', '收益', '的', '土地', '，', '不准', '荒芜', '。']]\n",
      "86924\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "train_data_txt = \"../training/msr_training.txt\"\n",
    "init_train_data = []\n",
    "with open(train_data_txt,encoding='gbk') as trt:\n",
    "    init_train_data = trt.readlines()\n",
    "all_data = \"\".join(init_train_data)\n",
    "train_data_split = [line.strip().split() for line in init_train_data]\n",
    "\n",
    "print(train_data_split[0:5])\n",
    "print(len(train_data_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNK>', '!', '#', '%', '&', \"'\", '(', ')', '*']\n"
     ]
    }
   ],
   "source": [
    "# 读取字符词典文件\n",
    "char_vocab_path = \"../training/char_vocabs.txt\"\n",
    "special_words = ['<PAD>', '<UNK>'] # 特殊词表示\n",
    "with open(char_vocab_path, \"r\", encoding=\"utf8\") as fo:\n",
    "    char_vocabs = [line.strip() for line in fo]\n",
    "char_vocabs = special_words + char_vocabs\n",
    "print(char_vocabs[0:10])\n",
    "\n",
    "# 字符和索引编号对应\n",
    "idx2vocab = {idx: char for idx, char in enumerate(char_vocabs)}\n",
    "vocab2idx = {char: idx for idx, char in idx2vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"BMES\"标记的标签\n",
    "label2idx = {\"S\": 0,\n",
    "             \"B\": 1, \n",
    "             \"M\": 2,\n",
    "             \"E\": 3\n",
    "             }\n",
    "# 索引和BMES标签对应\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取data\n",
    "def data_list(path, vocab2idx, label2idx):\n",
    "    with open(train_data_txt,encoding='gbk') as trt:\n",
    "        lines = trt.readlines()\n",
    "    datas, labels= [], []\n",
    "    for line in lines: # line是一个字符串\n",
    "        mline = line.strip().split() #mline是个词列表\n",
    "        sent, tag = [],[]\n",
    "        for i,word in enumerate(mline):\n",
    "            if len(word)==1:\n",
    "                sent.append(word)\n",
    "                tag.append(\"S\")\n",
    "            else:\n",
    "                s_word = [\"M\" for i in word]\n",
    "                s_word[0] = \"B\"\n",
    "                s_word[-1] = \"E\"\n",
    "                sent+=list(word)\n",
    "                tag+=s_word\n",
    "\n",
    "        sent_ids = [vocab2idx[char] if char in vocab2idx else vocab2idx['<UNK>'] for char in sent]\n",
    "        tag_ids = [label2idx[label] if label in label2idx else 0 for label in tag]\n",
    "        datas.append(sent_ids)\n",
    "        labels.append(tag_ids)\n",
    "\n",
    "    return datas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86924 86924\n",
      "[122, 5800, 6506, 563, 2572, 240, 3007, 2530, 6209, 6440, 6802, 3007, 2530, 3903, 2102, 2065, 6209, 6440, 182]\n",
      "['“', '这', '首', '先', '是', '个', '民', '族', '问', '题', '，', '民', '族', '的', '感', '情', '问', '题', '。']\n",
      "[0, 0, 1, 3, 0, 0, 1, 3, 1, 3, 0, 1, 3, 0, 1, 3, 1, 3, 0]\n",
      "['S', 'S', 'B', 'E', 'S', 'S', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'S']\n"
     ]
    }
   ],
   "source": [
    "datas, labels = data_list(train_data_txt, vocab2idx, label2idx)\n",
    "print(len(datas),len(labels))\n",
    "print(datas[5])\n",
    "print([idx2vocab[idx] for idx in datas[5]])\n",
    "print(labels[5])\n",
    "print([idx2label[idx] for idx in labels[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 3, 0, 0, 1, 3, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 2, 2, 3, 0, 1, 2, 3, 0, 0, 1, 3, 0, 1, 2, 2, 3, 0, 0, 0, 0, 1, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 2, 3, 1, 3, 0, 0, 1, 3, 1, 3, 1, 3, 0, 1, 3, 1, 3, 1, 3, 1, 3, 0, 1, 2, 2, 3, 0, 0, 1, 3, 1, 3, 0, 0, 1, 3, 0, 1, 3, 1, 2, 2, 3, 1, 3, 0, 1, 3, 0, 1, 3, 0, 1, 2, 2, 3, 1, 3, 0], [0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 3, 0, 1, 3, 0, 1, 3, 1, 3, 0], [0, 0, 1, 3, 0, 0, 1, 3, 1, 3, 0, 1, 3, 0, 1, 3, 1, 3, 0], [0, 0, 0, 0, 1, 3, 1, 2, 3, 0, 0, 1, 2, 3, 0, 0, 1, 3, 0], [0, 1, 3, 1, 3, 1, 3, 0, 1, 2, 3, 1, 3, 0, 1, 2, 3, 1, 2, 2, 3, 0, 1, 3, 0, 1, 3, 0], [0, 1, 3, 1, 3, 0, 0, 1, 3, 1, 3, 0, 1, 3, 0, 1, 3, 1, 3, 0, 0, 0, 1, 3, 0, 1, 3, 0, 1, 3, 1, 3, 0, 0], [0, 0, 1, 3, 0, 1, 3, 1, 3, 0, 1, 3, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "snum = 70000\n",
    "train_datas = datas[0:snum]\n",
    "train_labels = labels[0:snum]\n",
    "test_datas = datas[snum:]\n",
    "test_labels = labels[snum:]\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding sequences\n",
      "x_train shape: (70000, 100)\n",
      "x_test shape: (16924, 100)\n",
      "trainlabels shape: (70000, 100)\n",
      "testlabels shape: (16924, 100)\n",
      "trainlabels shape: (70000, 100, 5)\n",
      "testlabels shape: (16924, 100, 5)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 128)          880000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 128)          49280     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 5)            645       \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 100, 5)            16        \n",
      "=================================================================\n",
      "Total params: 1,028,501\n",
      "Trainable params: 1,028,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 63000 samples, validate on 7000 samples\n",
      "Epoch 1/2\n",
      "63000/63000 [==============================] - 177s 3ms/step - loss: 9.6692 - accuracy: 0.9400 - val_loss: 4.8137 - val_accuracy: 0.9581\n",
      "Epoch 2/2\n",
      "45792/63000 [====================>.........] - ETA: 46s - loss: 3.6246 - accuracy: 0.9658"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5b920d5c0007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m               \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 用crf自带的accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m              )\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking, Embedding, Bidirectional, LSTM, Dense, Input, TimeDistributed, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "from crf_keras import CRF\n",
    "from keras.layers import Dense, Embedding, Conv1D, Input\n",
    "from keras.models import Model # 这里我们学习使用Model型的模型\n",
    "import keras.backend as K # 引入Keras后端来自定义loss，注意Keras模型内的一切运算\n",
    "                          # 必须要通过Keras后端完成，比如取对数要用K.log不能用np.log\n",
    "\n",
    "embedding_size = 128\n",
    "EPOCHS = 2\n",
    "MAX_LEN=100\n",
    "CLASS_NUMS=5\n",
    "print('padding sequences')\n",
    "train_datas = sequence.pad_sequences(train_datas, maxlen=MAX_LEN)\n",
    "train_labels = sequence.pad_sequences(train_labels, maxlen=MAX_LEN)\n",
    "test_datas = sequence.pad_sequences(test_datas, maxlen=MAX_LEN)\n",
    "test_labels = sequence.pad_sequences(test_labels, maxlen=MAX_LEN)\n",
    "print('x_train shape:', train_datas.shape)\n",
    "print('x_test shape:', test_datas.shape)\n",
    "print('trainlabels shape:', train_labels.shape)\n",
    "print('testlabels shape:', test_labels.shape)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, CLASS_NUMS)#.reshape((70000,100, 5))\n",
    "test_labels = keras.utils.to_categorical(test_labels, CLASS_NUMS)#.reshape((16924,100, 5))\n",
    "print('trainlabels shape:', train_labels.shape)\n",
    "print('testlabels shape:', test_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "sequence = Input(shape=(None,), dtype='int32') # 建立输入层，输入长度设为None\n",
    "embedding = Embedding(len(char_vocabs)+1,\n",
    "                      embedding_size,\n",
    "                     )(sequence) # 去掉了mask_zero=True\n",
    "cnn = Conv1D(128, 3, activation='relu', padding='same')(embedding)\n",
    "cnn = Conv1D(128, 3, activation='relu', padding='same')(cnn)\n",
    "cnn = Conv1D(128, 3, activation='relu', padding='same')(cnn) # 层叠了3层CNN\n",
    "\n",
    "crf = CRF(True) # 定义crf层，参数为True，自动mask掉最后一个标签\n",
    "tag_score = Dense(5)(cnn) # 变成了5分类，第五个标签用来mask掉\n",
    "tag_score = crf(tag_score) # 包装一下原来的tag_score\n",
    "\n",
    "model = Model(inputs=sequence, outputs=tag_score)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=crf.loss, # 用crf自带的loss\n",
    "              optimizer='adam',\n",
    "              metrics=[crf.accuracy] # 用crf自带的accuracy\n",
    "             )\n",
    "model.fit(train_datas, train_labels, epochs=EPOCHS, verbose=1, validation_split=0.1)\n",
    "\n",
    "\n",
    "model.save_weights(\"../model/CNN_seg_weight.h5\")\n",
    "model.save(\"../model/CNN_seg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16924/16924 [==============================] - 3s 182us/step\n",
      "['loss', 'acc']\n",
      "[0.07513406304314581, 0.9746620172982068]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "score = model.evaluate(test_datas, test_labels, batch_size=BATCH_SIZE)\n",
    "print(model.metrics_names)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_in_dict(d): # 定义一个求字典中最大值的函数\n",
    "    key,value = list(d.items())[0]\n",
    "    for i,j in list(d.items())[1:]:\n",
    "        if j > value:\n",
    "            key,value = i,j\n",
    "    return key,value\n",
    "\n",
    "def viterbi(nodes, trans): # viterbi算法，跟前面的HMM一致\n",
    "    paths = nodes[0] # 初始化起始路径\n",
    "    for l in range(1, len(nodes)): # 遍历后面的节点\n",
    "        paths_old,paths = paths,{}\n",
    "        for n,ns in nodes[l].items(): # 当前时刻的所有节点\n",
    "            max_path,max_score = '',-1e10\n",
    "            for p,ps in paths_old.items(): # 截止至前一时刻的最优路径集合\n",
    "                score = ns + ps + trans[p[-1]+n] # 计算新分数\n",
    "                if score > max_score: # 如果新分数大于已有的最大分\n",
    "                    max_path,max_score = p+n, score # 更新路径\n",
    "            paths[max_path] = max_score # 储存到当前时刻所有节点的最优路径\n",
    "#     print(paths)\n",
    "    return max_in_dict(paths)\n",
    "\n",
    "\n",
    "def cut(s, trans): # 分词函数，也跟前面的HMM基本一致\n",
    "    if not s: # 空字符直接返回\n",
    "        return []\n",
    "    # 字序列转化为id序列。注意，经过我们前面对语料的预处理，字符集是没有空格的，\n",
    "    # 所以这里简单将空格的id跟句号的id等同起来\n",
    "    sent_ids = np.array([[vocab2idx.get(c, 0) if c != ' ' else vocab2idx[u'。']\n",
    "                          for c in s]])\n",
    "    probas = model.predict(sent_ids)[0] # 模型预测\n",
    "    nodes = [dict(zip('SBME', i)) for i in probas[:, :4]] # 只取前4个\n",
    "    nodes[0] = {i:j for i,j in nodes[0].items() if i in 'BS'} # 首字标签只能是b或s\n",
    "    nodes[-1] = {i:j for i,j in nodes[-1].items() if i in 'ES'} # 末字标签只能是e或s\n",
    "    tags = viterbi(nodes, trans)[0]\n",
    "    result = [s[0]]\n",
    "    for i,j in zip(s[1:], tags[1:]):\n",
    "        if j in 'BS': # 词的开始\n",
    "            result.append(i)\n",
    "        else: # 接着原来的词\n",
    "            result[-1] += i\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['储', '存到', '当前', '时刻', '所有', '节点', '的最', '优路', '径.']\n"
     ]
    }
   ],
   "source": [
    "t_ = model.get_weights()[-1][:4,:4] # 从训练模型中取出最新得到的转移矩阵\n",
    "# t_ = np.array([[ 0.2551752,   1.3712068,  -1.969512,   -0.94762987],\n",
    "#       [-1.3502188,  -0.11775374,  0.6775522,   2.1938992 ],\n",
    "#       [-2.6458318,  -0.72990716,  1.816428,    0.97933584],\n",
    "#       [ 0.73843807,  1.0811784,  -1.9546679,  -0.7160042 ]])\n",
    "\n",
    "trans = {}\n",
    "for i in 'SBME':\n",
    "    for j in 'SBME':\n",
    "        trans[i+j] = t_[label2idx[i], label2idx[j]]\n",
    "result = cut(\"储存到当前时刻所有节点的最优路径.\", trans)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2551752   1.3712068  -1.969512   -0.94762987]\n",
      " [-1.3502188  -0.11775374  0.6775522   2.1938992 ]\n",
      " [-2.6458318  -0.72990716  1.816428    0.97933584]\n",
      " [ 0.73843807  1.0811784  -1.9546679  -0.7160042 ]]\n"
     ]
    }
   ],
   "source": [
    "t_ = model.get_weights()[-1][:4,:4] \n",
    "print(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.3534518e-01 1.4596057e-01 7.9223752e-02 3.8083300e-02]\n",
      " [6.3772547e-01 1.5958913e-01 3.3011988e-02 1.6931488e-01]\n",
      " [4.3838363e-02 8.7163794e-01 6.9017991e-02 1.5349535e-02]\n",
      " [3.9679520e-02 3.2943442e-02 4.7815818e-02 8.7947863e-01]\n",
      " [7.2943561e-02 8.5752940e-01 1.4630562e-02 5.4879654e-02]\n",
      " [2.9200837e-01 9.3740760e-04 1.6958232e-03 7.0535827e-01]\n",
      " [7.3800108e-04 9.9867404e-01 1.0722456e-04 4.8074606e-04]\n",
      " [1.7609337e-04 9.1550837e-04 3.6390350e-04 9.9854457e-01]\n",
      " [9.9484384e-01 1.5055639e-04 2.2830433e-04 4.7772792e-03]\n",
      " [1.5624326e-03 9.9740249e-01 1.0211968e-03 1.3845533e-05]\n",
      " [1.1583652e-03 9.8706312e-07 7.0626709e-05 9.9876994e-01]\n",
      " [9.9908912e-01 8.8811549e-04 1.9136700e-05 3.5502653e-06]\n",
      " [9.7760838e-01 1.9476980e-02 2.5318420e-04 2.6613877e-03]\n",
      " [9.2452709e-03 9.6629715e-01 2.2554133e-02 1.9029514e-03]\n",
      " [1.1260571e-02 1.0427126e-05 1.0238566e-04 9.8862666e-01]\n",
      " [9.9973387e-01 2.2136088e-04 3.9861356e-05 5.0519270e-06]\n",
      " [5.7004488e-01 4.2899254e-01 1.2667493e-04 8.3547673e-04]\n",
      " [1.6749281e-01 3.7266701e-01 4.5703650e-02 4.1380966e-01]]\n",
      "最优状态路径为: [1. 1. 2. 4. 2. 2. 2. 4. 1. 2. 4. 1. 1. 2. 4. 1. 1. 2.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.609243   -0.60307544 -0.5321975  -1.350059  ]\n",
      " [ 2.1912084  -0.50220716 -1.9891578  -1.5365913 ]\n",
      " [-1.2706817   0.44745424  0.21643111 -0.7846817 ]\n",
      " [-0.79834855 -2.2858722  -0.7888813   2.3905082 ]\n",
      " [-0.4829742   1.3791542  -0.72924024 -1.4593649 ]\n",
      " [-0.21598284 -4.392511   -1.1265367   3.3144002 ]\n",
      " [-2.0698733   2.1803555  -0.8679491  -0.8824459 ]\n",
      " [-1.6855692  -1.5468001  -1.2390535   2.8636303 ]\n",
      " [ 3.8489604  -4.0107965  -0.8362013  -1.6430526 ]\n",
      " [ 0.26872087  1.424837   -0.645475   -3.6561563 ]\n",
      " [ 0.7524922  -3.4886906  -1.6522187   2.5781956 ]\n",
      " [ 4.5253954  -1.041991    0.11846626 -5.8860846 ]\n",
      " [ 4.114442   -5.469349   -1.738103   -0.07103527]\n",
      " [-1.6244731   2.3750694  -0.2490682  -3.1855345 ]\n",
      " [ 1.146612   -5.647822   -2.6322265   4.5583663 ]\n",
      " [ 5.8469195  -1.4046049   0.5016388  -8.909219  ]\n",
      " [ 2.5169191   1.220266   -2.5986495  -2.5697556 ]\n",
      " [ 0.16200602 -1.077709   -0.27034223  0.06126173]]\n",
      "最优状态路径为: [1. 3. 1. 3. 4. 2. 1. 4. 2. 4. 2. 4. 2. 4. 2. 4. 1. 3.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2551752  -1.3502188  -2.6458318   0.73843807]\n",
      " [ 1.3712068  -0.11775374 -0.72990716  1.0811784 ]\n",
      " [-1.969512    0.6775522   1.816428   -1.9546679 ]\n",
      " [-0.94762987  2.1938992   0.97933584 -0.7160042 ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
